## Описание проекта

Проект реализует ETL-процесс для аналитики, включая обработку данных о сессиях и хитах, их хранение в PostgreSQL и автоматизацию пайплайна с использованием Apache Airflow.

## Ключевые особенности и последние улучшения

- **Оптимизация производительности**:
  - Рефакторинг обработки данных с использованием Parquet для повышения эффективности
  - Сокращение времени выполнения DAG с 14 до 11 минут
  - Улучшена нормализация строк и типизация данных
  - Отказ от использования `map` и `apply` для более быстрой обработки данных

## Описание процессов проекта

### 1. **Подготовка данных**  
   Основной целью этого этапа является обеспечение качества и целостности данных перед их загрузкой в базу данных.  

   - **Обработка исходных данных**:  
     Данные поступают в двух форматах: PKL и JSON.  
     - PKL-файлы преобразованы в Parquet для оптимизации производительности DAG и эффективной работы с большими объемами данных. 
     - JSON-файлы применяются для ежедневного пополнения данных.  

     Данные проходят следующие этапы:
     - Преобразование дат, проверка типов данных.
     - Заполнение пропусков. Например, в атрибутах `device_brand` и `device_model` устанавливается значение `'unknown'` при отсутствии данных.
     - Удаление лишних или некорректных записей.

   - **Создание таблиц в БД**:  
     При первом запуске модуля проверяется наличие необходимых таблиц в базе данных.  
     Если они отсутствуют, выполняется их создание с помощью модуля `create_tables.py`.  
     Таблицы:
     - `sessions`: данные о сессиях, такие как источник, устройство, дата.
     - `hits`: взаимодействия пользователя с сайтом, включая события.

### 2. **Загрузка данных в БД**  
   Цель этого этапа — сохранить данные в удобной для анализа структуре.  

   - **Массовая загрузка данных**:  
     Используется функция `bulk_insert` из модуля `data_pipeline.py`. Данные загружаются в таблицы PostgreSQL пакетами, что увеличивает производительность.  
     - В таблицу `sessions` добавляются новые данные, дубликаты обновляются.
     - В таблицу `hits` загружаются данные о взаимодействиях с проверкой ссылочной целостности.

   - **Проверка целостности данных**:  
     Перед загрузкой хитов проверяется наличие соответствующих сессий. Это предотвращает ошибки и обеспечивает корректность данных.

### 3. **Автоматизация обработки**  
   Этот этап обеспечивает регулярную и автоматическую обработку новых данных.  

   - **DAG в Airflow**:  
     DAG (`sberauto_dag.py`) автоматизирует выполнение задач и включает в себя:  
     
     1. **Обработку данных из Parquet-файлов**:  
        Функция `insert_data` обрабатывает исторические данные и передает их в БД.  

     2. **Загрузку новых JSON-файлов**:  
        Функция `process_and_load_json_data` из модуля `json_to_db.py` обрабатывает файлы и загружает их в соответствующие таблицы.  

   - **Логирование и мониторинг**:  
     Все действия логируются, что позволяет быстро выявлять и исправлять ошибки.

   - **Гибкость DAG**:  
     DAG автоматически обновляет записи в БД, что гарантирует актуальность данных.

### Итог  
Проект обеспечивает эффективную обработку и загрузку данных веб-аналитики, автоматизирует ежедневные операции и предоставляет надежный инструмент для анализа.

## Структура проекта

```plaintext
dags/
    sberauto_dag.py       # Основной DAG для управления пайплайном в Airflow.
    
json_data/                # JSON файлы для ежедневной загрузки такого формата:
                            ga_hits_new_2022-01-01.json
                            ga_sessions_new_2022-01-01.json 

modules/
    create_tables.py      # Скрипт для создания таблиц в базе данных.
    data_pipeline.py      # Модуль для массовой загрузки данных в PostgreSQL.
    json_to_db.py         # Обработка JSON и загрузка в БД.

config/
    db_config.py          # Конфигурации для подключения к базе данных.
    user_pass.py          # Хранение учетных данных для подключения к БД.
    validation_cfg.yaml   # Настройки для проверки и приведения типов данных.

notebook/                 
    data_format_optimization_benchm.ipynb  # Преймущество Parquet над Pickle
```

## Использование

Проект адаптирован для локального запуска с PostgreSQL и Airflow.

1. **Клонируйте репозиторий**:
   ```bash
   git clone git@github.com:temeka1221/sberauto.git
   ```

2. **Настройте окружение**:
   - Убедитесь, что в системе установлены и настроены PostgreSQL и Apache Airflow.
   - В модуле `user_pass.py` задайте свои учетные данные для PostgreSQL:
     ```python
     DB_USER = 'your_user'
     DB_PASSWORD = 'your_password'
     ```
3. **Копирование DAG**:
   ```bash
   cp dags/sberauto_dag.py ~/airflow/dags/sberauto_dag.py
   ```

4. **Запустите Airflow**:
   ```bash
   airflow webserver -p 8090
   airflow scheduler
   ```

5. **Перейдите в Airflow**:  
   Откройте [http://localhost:8090/](http://localhost:8090/) и активируйте DAG `sberauto_dag`.


## Требования

- Python 3.x+
- PostgreSQL
- Apache Airflow
- Зависимости из `requirements.txt`

## Автор

Кокорев Артем | temeka

## Лицензия

Этот проект лицензирован на условиях MIT. Смотрите файл LICENSE.
